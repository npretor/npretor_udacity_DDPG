{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a3d41a-aa86-4497-b6e3-a4bf0a29f386",
   "metadata": {},
   "source": [
    "<b>Weight decay, taken from <a href=\"https://stackoverflow.com/questions/44452571/what-is-the-proper-way-to-weight-decay-for-adam-optimizer\">here</a></b>\n",
    "\n",
    "When using pure SGD (without momentum) as an optimizer, weight decay is the same thing as adding a L2-regularization term to the loss. When using any other optimizer, this is not true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed33f940-d75d-4eaa-9cdb-6436243fce36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb761cf3-212b-48ad-b4c5-5c0a959949d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "%matplotlib inline\n",
    "from ddpg_agent import Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f0945cb-553e-4288-859a-d4e01e7a21b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents:     1\n",
      "Size of each action:  4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like:   [[ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "   1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "   5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  -1.68164849e-01]]\n"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = Enviroment initialization = = = = = = = = = # \n",
    "env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1] \n",
    "\n",
    "print('Number of agents:    ', num_agents)  \n",
    "print('Size of each action: ', action_size) \n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size)) \n",
    "print('The state for the first agent looks like:  ', states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6799c7e9-50e4-432e-9eed-e587c7faedca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:    0.0\n",
      "Observations:    [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n",
      "Done status:     False\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=len(states[0]), action_size=action_size, random_seed=10) \n",
    "agent_num = 0 \n",
    "print(\"Rewards:   \",env_info.rewards[agent_num]) \n",
    "print(\"Observations:   \",env_info.vector_observations[agent_num] ) \n",
    "print(\"Done status:    \",env_info.local_done[agent_num] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d9a48-1a3f-4235-96c9-0fbcd7221391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10\t Average score: 0.09299999792128802\t Score: 0.7099999841302633\n",
      "Episode: 20\t Average score: 0.14999999664723873\t Score: 0.07999999821186066\n",
      "Episode: 30\t Average score: 0.19499999564141035\t Score: 0.0\n",
      "Episode: 40\t Average score: 0.1677499962504953\t Score: 0.0\n",
      "Episode: 50\t Average score: 0.1599999964237213\t Score: 0.0\n",
      "Episode: 60\t Average score: 0.1333333303531011\t Score: 0.0\n",
      "Episode: 70\t Average score: 0.11585714026753391\t Score: 0.1099999975413084\n",
      "Episode: 80\t Average score: 0.10137499773409217\t Score: 0.0\n",
      "Episode: 90\t Average score: 0.09011110909697083\t Score: 0.0\n",
      "Episode: 100\t Average score: 0.08109999818727374\t Score: 0.0\n",
      "Episode: 100\t Average score: 0.08109999818727374\n",
      "Episode: 110\t Average score: 0.07179999839514493\t Score: 0.0\n",
      "Episode: 120\t Average score: 0.05329999880865216\t Score: 0.0\n",
      "Episode: 130\t Average score: 0.024799999445676804\t Score: 0.0\n",
      "Episode: 140\t Average score: 0.016799999624490736\t Score: 0.0\n",
      "Episode: 150\t Average score: 0.0048999998904764655\t Score: 0.0\n",
      "Episode: 160\t Average score: 0.005699999872595072\t Score: 0.0\n",
      "Episode: 170\t Average score: 0.004599999897181988\t Score: 0.0\n",
      "Episode: 180\t Average score: 0.004599999897181988\t Score: 0.0\n",
      "Episode: 190\t Average score: 0.004599999897181988\t Score: 0.0\n",
      "Episode: 200\t Average score: 0.004599999897181988\t Score: 0.0\n",
      "Episode: 200\t Average score: 0.004599999897181988\n",
      "Episode: 210\t Average score: 0.005399999879300594\t Score: 0.0\n",
      "Episode: 220\t Average score: 0.0040999999083578586\t Score: 0.0\n",
      "Episode: 230\t Average score: 0.0048999998904764655\t Score: 0.0\n",
      "Episode: 240\t Average score: 0.0042999999038875105\t Score: 0.0\n",
      "Episode: 250\t Average score: 0.003299999926239252\t Score: 0.0\n",
      "Episode: 260\t Average score: 0.003399999924004078\t Score: 0.0\n",
      "Episode: 270\t Average score: 0.004499999899417162\t Score: 0.0\n",
      "Episode: 280\t Average score: 0.004499999899417162\t Score: 0.0\n",
      "Episode: 290\t Average score: 0.0046999998949468135\t Score: 0.0\n",
      "Episode: 300\t Average score: 0.0046999998949468135\t Score: 0.0\n",
      "Episode: 300\t Average score: 0.0046999998949468135\n",
      "Episode: 310\t Average score: 0.003899999912828207\t Score: 0.0\n",
      "Episode: 320\t Average score: 0.0029999999329447745\t Score: 0.0\n",
      "Episode: 330\t Average score: 0.002199999950826168\t Score: 0.0\n",
      "Episode: 340\t Average score: 0.002199999950826168\t Score: 0.0\n",
      "Episode: 350\t Average score: 0.002199999950826168\t Score: 0.0\n",
      "Episode: 360\t Average score: 0.0012999999709427358\t Score: 0.0\n",
      "Episode: 370\t Average score: 0.00019999999552965163\t Score: 0.0\n",
      "Episode: 380\t Average score: 0.00019999999552965163\t Score: 0.0\n",
      "Episode: 390\t Average score: 0.0005999999865889549\t Score: 0.0\n",
      "Episode: 400\t Average score: 0.0005999999865889549\t Score: 0.0\n",
      "Episode: 400\t Average score: 0.0005999999865889549\n",
      "Episode: 410\t Average score: 0.001699999962002039\t Score: 0.0\n",
      "Episode: 420\t Average score: 0.002799999937415123\t Score: 0.0\n",
      "Episode: 430\t Average score: 0.002799999937415123\t Score: 0.0\n",
      "Episode: 440\t Average score: 0.002799999937415123\t Score: 0.0\n",
      "Episode: 450\t Average score: 0.004999999888241291\t Score: 0.0\n",
      "Episode: 460\t Average score: 0.004999999888241291\t Score: 0.0\n",
      "Episode: 470\t Average score: 0.004999999888241291\t Score: 0.0\n",
      "Episode: 480\t Average score: 0.005999999865889549\t Score: 0.0\n",
      "Episode: 490\t Average score: 0.005399999879300594\t Score: 0.0\n",
      "Episode: 500\t Average score: 0.0064999998547136785\t Score: 0.0\n",
      "Episode: 500\t Average score: 0.0064999998547136785\n",
      "Episode: 510\t Average score: 0.0064999998547136785\t Score: 0.0\n",
      "Episode: 520\t Average score: 0.005399999879300594\t Score: 0.0\n",
      "Episode: 530\t Average score: 0.005399999879300594\t Score: 0.0\n",
      "Episode: 540\t Average score: 0.005399999879300594\t Score: 0.0\n",
      "Episode: 550\t Average score: 0.003199999928474426\t Score: 0.0\n",
      "Episode: 560\t Average score: 0.0042999999038875105\t Score: 0.0\n",
      "Episode: 570\t Average score: 0.0042999999038875105\t Score: 0.0\n",
      "Episode: 580\t Average score: 0.003299999926239252\t Score: 0.0\n",
      "Episode: 590\t Average score: 0.003299999926239252\t Score: 0.0\n",
      "Episode: 600\t Average score: 0.002199999950826168\t Score: 0.0\n",
      "Episode: 600\t Average score: 0.002199999950826168\n",
      "Episode: 610\t Average score: 0.001099999975413084\t Score: 0.0\n",
      "Episode: 620\t Average score: 0.003299999926239252\t Score: 0.0\n",
      "Episode: 630\t Average score: 0.004999999888241291\t Score: 0.0\n",
      "Episode: 640\t Average score: 0.004999999888241291\t Score: 0.0\n",
      "Episode: 650\t Average score: 0.004999999888241291\t Score: 0.0\n",
      "Episode: 660\t Average score: 0.0048999998904764655\t Score: 0.0\n",
      "Episode: 670\t Average score: 0.0048999998904764655\t Score: 0.0\n",
      "Episode: 680\t Average score: 0.005399999879300594\t Score: 0.0\n",
      "Episode: 690\t Average score: 0.005399999879300594\t Score: 0.0\n",
      "Episode: 700\t Average score: 0.005399999879300594\t Score: 0.0\n",
      "Episode: 700\t Average score: 0.005399999879300594\n"
     ]
    }
   ],
   "source": [
    "def ddpg(num_episodes, max_timesteps=500):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    max_score = -np.Inf \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    \n",
    "    for ith_episode in range(1, num_episodes+1): \n",
    "        agent.reset() \n",
    "        score = 0 \n",
    "\n",
    "        for timestep in range(max_timesteps): \n",
    "            action = agent.act(state) \n",
    "\n",
    "            env_info = env.step(action) \n",
    "            next_state = env_info[brain_name].vector_observations[agent_num]\n",
    "            reward = env_info[brain_name].rewards[agent_num]\n",
    "            done = env_info[brain_name].local_done[agent_num] \n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done) \n",
    "\n",
    "            state = next_state \n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores_deque.append(score) \n",
    "        scores.append(score) \n",
    "        \n",
    "        if ith_episode % 10 == 0:        \n",
    "            print(\"Episode: {}\\t Average score: {}\\t Score: {}\".format(ith_episode, np.mean(scores_deque), score)) \n",
    "\n",
    "        if ith_episode % 100 == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), \"checkpoint_actor.pth\")   \n",
    "            torch.save(agent.critic_local.state_dict(), \"checkpoint_critic.pth\") \n",
    "            print(\"Episode: {}\\t Average score: {}\".format(ith_episode, np.mean(scores_deque))) \n",
    "\n",
    "    return scores \n",
    "\n",
    "scores = ddpg(num_episodes=2000) \n",
    "\n",
    "fig = plt.figure() \n",
    "ax = fig.add_subplot(111) \n",
    "plt.plot(np.arange(1, len(scores)+1), scores) \n",
    "plt.ylabel('Score') \n",
    "plt.xlabel('Episode #') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98808f34-3636-432a-a845-13e502970018",
   "metadata": {},
   "source": [
    "### Default values: \n",
    "- BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "- BATCH_SIZE = 128        # minibatch size\n",
    "- GAMMA = 0.99            # discount factor\n",
    "- TAU = 1e-3              # for soft update of target parameters\n",
    "- LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "- LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "- WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "\n",
    "1. Cancelled after 700 timesteps, no change\n",
    "\n",
    "2. Increased max_timesteps to 1000. No change after 130 episodes \n",
    "\n",
    "3. Changed: max_timesteps=500, LR_actor = 1e-3, LR_CRITIC=3e-3. Switched to GPU\n",
    "\n",
    "4. Default values, 1200 episides no change, 1000 timesteps \n",
    "\n",
    "5. Episodes: 2000, max_steps=500. Score shows up, avg hovers at 0.1 to 0.13\n",
    "    BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "    BATCH_SIZE = 128        # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0.01   # L2 weight decay\n",
    "    \n",
    "6. 1000 episodes, maxsteps=500 \n",
    "    No change in the average score after increasing the learning rates from 1e-4 to 1e-3 \n",
    "    \n",
    "7. 1000 episodes, maxsteps=700, learning rates are both 1e-3. Apparent increase \n",
    "\n",
    "8. 2000 episodes, max_timesteps=500\n",
    "- BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "- BATCH_SIZE = 128        # minibatch size\n",
    "- GAMMA = 0.99            # discount factor\n",
    "- TAU = 1e-3              # for soft update of target parameters\n",
    "- LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "- LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "- WEIGHT_DECAY = 0.1   # L2 weight decay\n",
    "\n",
    "9. Episodes=2000, max_timesteps=500. avg_score ends at 0.-14\n",
    "    BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "    BATCH_SIZE = 128        # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-3        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0.1   # L2 weight decay\n",
    "    \n",
    "10. Episodes: 600, max_timesteps=500. Increased from 0.08 to 0.12. Non-steady increase, peak in the middle at 0.15 \n",
    "    BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "    BATCH_SIZE = 128        # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0.1   # L2 weight decay \n",
    "    \n",
    "11. episodes: 5000, max_timesteps=500. No change \n",
    "    BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "    BATCH_SIZE = 128        # minibatch size\n",
    "    GAMMA = 0.999            # discount factor\n",
    "    TAU = 0.001              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-5         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-5        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0.1   # L2 weight decay \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "12. Very low average values that change slowly. Learning rate appears to be very slow\n",
    "    episodes: 800 \n",
    "    max_timesteps=500. \n",
    "    BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "    BATCH_SIZE = 128        # minibatch size\n",
    "    GAMMA = 0.999            # discount factor\n",
    "    TAU = 0.001              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-5         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0.001   # L2 weight decay \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "13. Intermittent negligible scores (0.003 avg) \n",
    "    episodes: 800 \n",
    "    max_timesteps=500. \n",
    "    BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "    BATCH_SIZE = 128        # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 0.001              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-5         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0.001   # L2 weight decay \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "14  Training crashed, kernel dead twice after 400 episodes. No learning above 0.004\n",
    "    max_timesteps=500. \n",
    "    BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "    BATCH_SIZE = 128        # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 0.01              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-3        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0.1   # L2 weight decay \n",
    "\n",
    "\n",
    "\n",
    "15 No learning above 0.0006, repeats of same values 10 episodes in a row. Stopped after 350 episodes \n",
    "    BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "    BATCH_SIZE = 128        # minibatch size\n",
    "    GAMMA = 0.95            # discount factor\n",
    "    TAU = 0.1              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-3        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0.1   # L2 weight decay \n",
    "\n",
    "\n",
    "\n",
    "16 Removed custom weight decay for Adam. Reached .19 after 30 episodes, dropped to 0.004 afterwards \n",
    "BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.995            # discount factor\n",
    "TAU = 1e-2              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 3e-3        # learning rate of the critic\n",
    "\n",
    "\n",
    "17. Trained for 9000 episodes. Initial bump at episode 30, not above 0.007 for several thousand. \n",
    "    BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "    BATCH_SIZE = 128        # minibatch size\n",
    "    GAMMA = 0.995            # discount factor\n",
    "    TAU = 1e-2              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-3        # learning rate of the critic\n",
    "\n",
    "\n",
    "18  Trained for 1200 episodes. Increase to .5 at 300 episodes, then steady drop. \n",
    "    BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "    BATCH_SIZE = 256        # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-3        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0   # L2 weight decay \n",
    "\n",
    "\n",
    "19  INCREASED BATCH SIZE TO 1000Trained for 1200 episodes. Increase to .5 at 300 episodes, then steady drop.\n",
    "\n",
    "\n",
    "20  Increased to .7 halfway, then descended to 0.05 by episode 1000, max_timesteps=2000 by accident \n",
    "    BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "    BATCH_SIZE = 512        # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-3        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0   # L2 weight decay \n",
    "\n",
    "\n",
    "21  Highest score yet. Increased to avg of 2.0 at 230 episodes, then quickly fell to 0.1 \n",
    "    BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "    BATCH_SIZE = 1024        # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-3        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0   # L2 weight decay \n",
    "\n",
    "\n",
    "22 othing after 130 episodes except a slow increase from 0.007 to 0.059\n",
    "max_timesteps=1000, episodes=1000\n",
    "BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "BATCH_SIZE = 1024        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-5         # learning rate of the actor \n",
    "LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0   # L2 weight decay \n",
    "\n",
    "\n",
    "23  Decreasing critic network size, removed third layer fc layer out of 4. Changing both learning rates to 1e-4. Killed at 100 episodes, too random\n",
    "max_timesteps=1000, episodes=1000\n",
    "BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "BATCH_SIZE = 1024        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-5         # learning rate of the actor \n",
    "LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0   # L2 weight decay \n",
    "\n",
    "\n",
    "24  LR for both =1e-3. No change or convergence\n",
    "max_timesteps=1000, episodes=1000\n",
    "BUFFER_SIZE = int(1000000)  # replay buffer size\n",
    "BATCH_SIZE = 1024        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 3e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0   # L2 weight decay \n",
    "\n",
    "25 No luck, no increase of average score over 1. Switching to weights and biases\n",
    "    \"BUFFER_SIZE\": int(1000000),     # replay buffer size \n",
    "    \"BATCH_SIZE\" : 1024,             # minibatch size \n",
    "    \"GAMMA\" : 0.99,                  # discount factor \n",
    "    \"TAU\" : 1e-3,                    # for soft update of target parameters \n",
    "    \"LR_ACTOR\" : 1e-4,               # learning rate of the actor   \n",
    "    \"LR_CRITIC\" : 3e-3,              # learning rate of the critic  \n",
    "    \"WEIGHT_DECAY\": 0 ,              # L2 weight decay\n",
    "    \"num_episodes\": 1000,\n",
    "    \"max_timesteps\": 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a964c92",
   "metadata": {},
   "source": [
    "### Session 21\n",
    "Episode: 10\t Average score: 0.2679999940097332\t Score: 0.0\n",
    "Episode: 20\t Average score: 0.4489999899640679\t Score: 0.6199999861419201\n",
    "Episode: 30\t Average score: 0.5416666545594732\t Score: 0.6099999863654375\n",
    "Episode: 40\t Average score: 0.7479999832808971\t Score: 1.219999972730875\n",
    "Episode: 50\t Average score: 0.8393999812379479\t Score: 1.0099999774247408\n",
    "Episode: 60\t Average score: 0.987499977927655\t Score: 0.23999999463558197\n",
    "Episode: 70\t Average score: 1.026714262765433\t Score: 1.339999970048666\n",
    "Episode: 80\t Average score: 1.1418749744771048\t Score: 1.2999999709427357\n",
    "Episode: 90\t Average score: 1.2231110837724475\t Score: 0.5099999886006117\n",
    "Episode: 100\t Average score: 1.2948999710567295\t Score: 4.619999896734953\n",
    "Episode: 100\t Average score: 1.2948999710567295\n",
    "Episode: 110\t Average score: 1.4751999670267104\t Score: 1.409999968484044\n",
    "Episode: 120\t Average score: 1.594299964364618\t Score: 3.05999993160367\n",
    "Episode: 130\t Average score: 1.663799962811172\t Score: 1.3799999691545963\n",
    "Episode: 140\t Average score: 1.7327999612689018\t Score: 2.0199999548494816\n",
    "Episode: 150\t Average score: 1.7736999603547157\t Score: 1.4199999682605267\n",
    "Episode: 160\t Average score: 1.726399961411953\t Score: 1.289999971166253\n",
    "Episode: 170\t Average score: 1.7759999603033065\t Score: 1.1599999740719795\n",
    "Episode: 180\t Average score: 1.8208999592997133\t Score: 1.50999996624887\n",
    "Episode: 190\t Average score: 1.8731999581307173\t Score: 2.229999950155616\n",
    "Episode: 200\t Average score: 1.8926999576948582\t Score: 0.9199999794363976\n",
    "Episode: 200\t Average score: 1.8926999576948582\n",
    "Episode: 210\t Average score: 1.920399957075715\t Score: 1.389999968931079\n",
    "Episode: 220\t Average score: 1.9735999558866024\t Score: 3.6199999190866947\n",
    "Episode: 230\t Average score: 2.027599954679608\t Score: 0.3499999921768904\n",
    "Episode: 240\t Average score: 1.9000999575294555\t Score: 0.8499999810010195\n",
    "Episode: 250\t Average score: 1.812499959487468\t Score: 1.0799999758601189\n",
    "Episode: 260\t Average score: 1.734899961221963\t Score: 0.0\n",
    "Episode: 270\t Average score: 1.602999964170158\t Score: 0.06999999843537807\n",
    "Episode: 280\t Average score: 1.4343999679386616\t Score: 0.2699999939650297\n",
    "Episode: 290\t Average score: 1.2123999729007482\t Score: 0.3199999928474426\n",
    "Episode: 300\t Average score: 1.010499977413565\t Score: 0.3899999912828207\n",
    "Episode: 300\t Average score: 1.010499977413565\n",
    "Episode: 310\t Average score: 0.8032999820448459\t Score: 0.04999999888241291\n",
    "Episode: 320\t Average score: 0.6005999865755439\t Score: 0.03999999910593033\n",
    "Episode: 330\t Average score: 0.41679999068379403\t Score: 0.0\n",
    "Episode: 340\t Average score: 0.34829999221488833\t Score: 0.0\n",
    "Episode: 350\t Average score: 0.2804999937303364\t Score: 0.35999999195337296\n",
    "Episode: 360\t Average score: 0.237699994686991\t Score: 0.0\n",
    "Episode: 370\t Average score: 0.19489999564364552\t Score: 0.0\n",
    "Episode: 380\t Average score: 0.13519999697804452\t Score: 0.0\n",
    "Episode: 390\t Average score: 0.12949999710544943\t Score: 0.0\n",
    "Episode: 400\t Average score: 0.12959999710321427\t Score: 0.0\n",
    "Episode: 400\t Average score: 0.12959999710321427\n",
    "Episode: 410\t Average score: 0.13269999703392388\t Score: 0.019999999552965164\n",
    "Episode: 420\t Average score: 0.10439999766647816\t Score: 0.0\n",
    "Episode: 430\t Average score: 0.10359999768435955\t Score: 0.0\n",
    "Episode: 440\t Average score: 0.09789999781176448\t Score: 0.0\n",
    "Episode: 450\t Average score: 0.1006999977491796\t Score: 0.0\n",
    "Episode: 460\t Average score: 0.10409999767318368\t Score: 0.0\n",
    "Episode: 470\t Average score: 0.12559999719262124\t Score: 1.1199999749660492\n",
    "Episode: 480\t Average score: 0.14619999673217535\t Score: 0.17999999597668648\n",
    "Episode: 490\t Average score: 0.18189999593421816\t Score: 0.4899999890476465\n",
    "Episode: 500\t Average score: 0.2118999952636659\t Score: 0.25999999418854713\n",
    "Episode: 500\t Average score: 0.2118999952636659\n",
    "Episode: 510\t Average score: 0.1982999955676496\t Score: 0.7099999841302633\n",
    "Episode: 520\t Average score: 0.21289999524131417\t Score: 0.0\n",
    "Episode: 530\t Average score: 0.23559999473392965\t Score: 0.0\n",
    "Episode: 540\t Average score: 0.23329999478533864\t Score: 0.0\n",
    "Episode: 550\t Average score: 0.23959999464452267\t Score: 0.0\n",
    "Episode: 560\t Average score: 0.31049999305978415\t Score: 0.6499999854713678\n",
    "Episode: 570\t Average score: 0.3582999919913709\t Score: 0.3799999915063381\n",
    "Episode: 580\t Average score: 0.43089999036863447\t Score: 0.8599999807775021\n",
    "Episode: 590\t Average score: 0.509799988605082\t Score: 0.9299999792128801\n",
    "Episode: 600\t Average score: 0.4960999889113009\t Score: 0.0\n",
    "Episode: 600\t Average score: 0.4960999889113009\n",
    "Episode: 610\t Average score: 0.5112999885715545\t Score: 0.0\n",
    "Episode: 620\t Average score: 0.5495999877154827\t Score: 0.0\n",
    "Episode: 630\t Average score: 0.5696999872662127\t Score: 1.1399999745190144\n",
    "Episode: 640\t Average score: 0.6477999855205416\t Score: 1.579999964684248\n",
    "Episode: 650\t Average score: 0.7190999839268625\t Score: 1.219999972730875\n",
    "Episode: 660\t Average score: 0.7593999830260872\t Score: 1.1399999745190144\n",
    "Episode: 670\t Average score: 0.7678999828360975\t Score: 0.3399999924004078\n",
    "Episode: 680\t Average score: 0.7648999829031528\t Score: 1.649999963119626\n",
    "Episode: 690\t Average score: 0.7514999832026661\t Score: 1.529999965801835\n",
    "Episode: 700\t Average score: 0.8196999816782773\t Score: 1.649999963119626\n",
    "Episode: 700\t Average score: 0.8196999816782773\n",
    "Episode: 710\t Average score: 0.8417999811843038\t Score: 0.7499999832361937\n",
    "Episode: 720\t Average score: 0.8165999817475676\t Score: 0.18999999575316906\n",
    "Episode: 730\t Average score: 0.803199982047081\t Score: 1.1799999736249447\n",
    "Episode: 740\t Average score: 0.7572999830730259\t Score: 0.3199999928474426\n",
    "Episode: 750\t Average score: 0.6740999849326909\t Score: 0.09999999776482582\n",
    "Episode: 760\t Average score: 0.5576999875344336\t Score: 0.0\n",
    "Episode: 770\t Average score: 0.478799989297986\t Score: 0.0\n",
    "Episode: 780\t Average score: 0.3917999912425876\t Score: 0.07999999821186066\n",
    "Episode: 790\t Average score: 0.3113999930396676\t Score: 0.1099999975413084\n",
    "Episode: 800\t Average score: 0.23299999479204417\t Score: 0.0\n",
    "Episode: 800\t Average score: 0.23299999479204417\n",
    "Episode: 810\t Average score: 0.1978999955765903\t Score: 0.1099999975413084\n",
    "Episode: 820\t Average score: 0.17729999603703617\t Score: 0.8599999807775021\n",
    "Episode: 830\t Average score: 0.14389999678358437\t Score: 0.08999999798834324\n",
    "Episode: 840\t Average score: 0.1374999969266355\t Score: 0.0\n",
    "Episode: 850\t Average score: 0.15539999652653932\t Score: 0.0\n",
    "Episode: 860\t Average score: 0.18179999593645335\t Score: 0.0\n",
    "Episode: 870\t Average score: 0.18419999588280916\t Score: 0.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
